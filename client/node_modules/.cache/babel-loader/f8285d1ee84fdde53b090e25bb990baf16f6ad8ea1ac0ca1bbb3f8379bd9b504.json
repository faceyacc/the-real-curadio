{"ast":null,"code":"import { Configuration, OpenAIApi } from 'openai';\nconst configuration = new Configuration({\n  apiKey: process.env.OPENAI_API_KEY\n});\nconst openai = new OpenAIApi(configuration);\nconst basePromptPrefix = `\nI am going to list my mood, a genre of music, time of day, and what I am currently doing. I want ChatGPT-3 to create the best playlist for me base on the aforementioned points:\n\nuser prompt:\n`;\nconst generateAction = async (req, res) => {\n  // Run first prompt\n  console.log(`API: ${basePromptPrefix}${req.body.userInput}`);\n  const baseCompletion = await openai.createCompletion({\n    model: 'text-davinci-003',\n    prompt: `${basePromptPrefix}${req.body.userInput}\\n`,\n    temperature: 0.7,\n    max_tokens: 250\n  });\n  const basePromptOutput = baseCompletion.data.choices.pop();\n\n  // Make call to shuffle using prompt chaining\n  // await shufflePlaylist(baseCompletion)\n\n  // Send over the prompt #2 output to UI instead of prompt #1\n  res.status(200).json({\n    output: basePromptOutput\n  });\n};\nexport default generateAction;","map":{"version":3,"names":["Configuration","OpenAIApi","configuration","apiKey","process","env","OPENAI_API_KEY","openai","basePromptPrefix","generateAction","req","res","console","log","body","userInput","baseCompletion","createCompletion","model","prompt","temperature","max_tokens","basePromptOutput","data","choices","pop","status","json","output"],"sources":["/Users/ty/projects/curio/client/src/generate.js"],"sourcesContent":["import { Configuration, OpenAIApi } from 'openai'\n\nconst configuration = new Configuration({\n    apiKey: process.env.OPENAI_API_KEY,\n});\n\nconst openai = new OpenAIApi(configuration)\n\nconst basePromptPrefix = \n`\nI am going to list my mood, a genre of music, time of day, and what I am currently doing. I want ChatGPT-3 to create the best playlist for me base on the aforementioned points:\n\nuser prompt:\n`\n\n\nconst generateAction = async (req, res) => {\n\n    // Run first prompt\n    console.log(`API: ${basePromptPrefix}${req.body.userInput}`)\n\n    const baseCompletion = await openai.createCompletion({\n        model: 'text-davinci-003',\n        prompt: `${basePromptPrefix}${req.body.userInput}\\n`,\n        temperature: 0.7,\n        max_tokens: 250,\n    })\n\n    const basePromptOutput = baseCompletion.data.choices.pop()\n\n    // Make call to shuffle using prompt chaining\n    // await shufflePlaylist(baseCompletion)\n\n    // Send over the prompt #2 output to UI instead of prompt #1\n    res.status(200).json({ output: basePromptOutput })\n};\n\n\nexport default generateAction;\n\n"],"mappings":"AAAA,SAASA,aAAa,EAAEC,SAAS,QAAQ,QAAQ;AAEjD,MAAMC,aAAa,GAAG,IAAIF,aAAa,CAAC;EACpCG,MAAM,EAAEC,OAAO,CAACC,GAAG,CAACC;AACxB,CAAC,CAAC;AAEF,MAAMC,MAAM,GAAG,IAAIN,SAAS,CAACC,aAAa,CAAC;AAE3C,MAAMM,gBAAgB,GACrB;AACD;AACA;AACA;AACA,CAAC;AAGD,MAAMC,cAAc,GAAG,OAAOC,GAAG,EAAEC,GAAG,KAAK;EAEvC;EACAC,OAAO,CAACC,GAAG,CAAE,QAAOL,gBAAiB,GAAEE,GAAG,CAACI,IAAI,CAACC,SAAU,EAAC,CAAC;EAE5D,MAAMC,cAAc,GAAG,MAAMT,MAAM,CAACU,gBAAgB,CAAC;IACjDC,KAAK,EAAE,kBAAkB;IACzBC,MAAM,EAAG,GAAEX,gBAAiB,GAAEE,GAAG,CAACI,IAAI,CAACC,SAAU,IAAG;IACpDK,WAAW,EAAE,GAAG;IAChBC,UAAU,EAAE;EAChB,CAAC,CAAC;EAEF,MAAMC,gBAAgB,GAAGN,cAAc,CAACO,IAAI,CAACC,OAAO,CAACC,GAAG,EAAE;;EAE1D;EACA;;EAEA;EACAd,GAAG,CAACe,MAAM,CAAC,GAAG,CAAC,CAACC,IAAI,CAAC;IAAEC,MAAM,EAAEN;EAAiB,CAAC,CAAC;AACtD,CAAC;AAGD,eAAeb,cAAc"},"metadata":{},"sourceType":"module","externalDependencies":[]}